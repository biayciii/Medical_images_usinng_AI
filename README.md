# Dual-Stream Late Fusion U-Net for Head and Neck Tumor Segmentation

This repository contains the implementation of a sophisticated Deep Learning framework designed for the precise segmentation of Organs at Risk (OARs), specifically the Brainstem, in the head and neck region. By leveraging a Dual-Stream Late Fusion U-Net architecture, the system directly processes and fuses multi-modal medical imaging (CT and T1-weighted MRI) without relying on synthetic data generation.

## System Architecture

The core of this project is the Dual-Stream mechanism. As CT and MRI represent fundamentally different physical properties, we keep their feature extraction independent before late-stage integration.

### Proposed Late Fusion Pipeline
![Proposed Architecture](image/CT-Trang1.drawio.png)

### Baseline Single-Channel U-Net (For Comparison)
![Baseline U-Net](image/baseline Unet.drawio (1).png)

## Key Innovations

* **Dual-Stream Architecture:** Features two specialized parallel encoding branches.
  * *CT Stream (Geometric Encoder):* Extracts rigid anatomical landmarks and bony boundaries.
  * *MRI Stream (Semantic Encoder):* Extracts soft-tissue semantic features and inter-organ contrast.
* **Late Fusion Mechanism:** Multi-modal information is merged via channel concatenation only at the network's deepest bottleneck ($128\times128$ resolution) to prevent feature interference from shallow layers.
* **Advanced Preprocessing Pipeline:**
  * CT Scans: Soft-tissue windowing ($WL=40$ HU, $WW=400$ HU) to eliminate bone/air noise.
  * MRI Scans: Percentile clipping (1st to 99th percentile) to suppress extreme bright artifacts.
  * Normalization: Min-Max normalization to a [0, 1] continuous range.

## Tackling Class Imbalance with Hybrid Loss

Medical image segmentation suffers from extreme class imbalance (target organs often occupy $<2\%$ of total voxels). To prevent the model from collapsing into a trivial local minimum, we implemented a custom Hybrid Loss strategy. 

The total objective function synergizes Binary Cross Entropy (BCE) for gradient stability and Dice Loss for boundary refinement:

$\mathcal{L}_{Total}=\lambda_{1}\mathcal{L}_{BCE}+\lambda_{2}\mathcal{L}_{Dice}$

## Dataset

The model is trained and validated on the public **HaN-Seg (Head and Neck Segmentation)** dataset, utilizing perfectly paired and spatially registered 3D scans from 42 patients. The 3D volumes were processed as 2D slices resized to $256\times256$ pixels.

## Performance & Quantitative Results

The Late Fusion approach demonstrated a significant breakthrough in accuracy compared to a standard single-modality Baseline U-Net.

| Model | Input Modality | Validation Dice Score |
| :--- | :--- | :--- |
| Baseline U-Net | CT Only | 96.58% |
| **Proposed U-Net** | **CT + MRI (Late Fusion)** | **97.43%** |

*Clinical Note: The model achieved these results on a strict patient-level split (unseen patients), confirming its high robustness and clinical viability.*

## Qualitative Visual Results

The visual outputs confirm the model's exceptional precision in localizing and delineating the brainstem with smooth, continuous contours that closely mirror expert annotations.

![Segmentation Visual Results](image/case_12_slice_100.png)
*From left to right: (1) Preprocessed CT input, (2) Preprocessed T1-weighted MRI input, (3) Expert-annotated Ground Truth mask, and (4) The Predicted Mask generated by our model.*

## Setup and Execution

```bash
git clone [https://github.com/your-username/han-seg-late-fusion.git](https://github.com/your-username/han-seg-late-fusion.git)
cd han-seg-late-fusion
pip install -r requirements.txt

# Run the training pipeline
python src/train.py --dataset han_seg_dir
