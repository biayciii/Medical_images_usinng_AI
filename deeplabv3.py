# -*- coding: utf-8 -*-
"""DeeplabV3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zF6DlCaeYQ71bTOlQo8PJobfY6y9MZDg
"""

from google.colab import drive
drive.mount('/content/drive')

"""LÃ¡t cáº¯t má»›i"""

!pip install pynrrd matplotlib

!pip install SimpleITK

import os
import SimpleITK as sitk

file_path = "/content/drive/MyDrive/validation/oncologist/TCGA-CV-5978/CT_IMAGE.nrrd"

# Äá»c áº£nh
image = sitk.ReadImage(file_path)
array = sitk.GetArrayFromImage(image)

# Äáº¿m sá»‘ lÆ°á»£ng slice
num_slices = array.shape[0]
print("Sá»‘ lÆ°á»£ng slice trong CT:", num_slices)

import matplotlib.pyplot as plt

folder_output = "/content/drive/MyDrive/Tep_moi/oncologist/TCGA-CV-5978"
os.makedirs(folder_output, exist_ok=True)

start_slice = 1
end_slice = min(144, num_slices)

print(f"LÆ°u slices tá»« {start_slice} Ä‘áº¿n {end_slice}")

for idx in range(start_slice, end_slice + 1):
    slice_img = array[idx]

    plt.figure(figsize=(6,6))
    plt.imshow(slice_img, cmap='gray')
    plt.title(f"Slice {idx}")
    plt.axis('off')

    save_path = os.path.join(folder_output, f"slice_{idx}.png")
    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)
    plt.close()

print("ÄÃ£ lÆ°u xong toÃ n bá»™ hÃ¬nh áº£nh táº¡i:", folder_output)

import nrrd
import numpy as np
import cv2
import os

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---
SEGMENTATION_FOLDER = '/content/drive/MyDrive/validation/oncologist/TCGA-CV-5978/segmentations'
OUTPUT_ROOT = '/content/drive/MyDrive/anh_mask/oncologist/TCGA-CV-5978'

def convert_nrrd_to_mask(nrrd_path, output_dir):
    try:
        print(f"   Äang Ä‘á»c file: {nrrd_path}")
        data, header = nrrd.read(nrrd_path)

        # Táº¡o thÆ° má»¥c Ä‘áº§u ra náº¿u chÆ°a cÃ³
        os.makedirs(output_dir, exist_ok=True)

        num_slices = data.shape[2]

        for i in range(num_slices):
            mask_slice = data[:, :, i]

            # Xá»­ lÃ½ xoay/láº­t áº£nh (giá»¯ nguyÃªn theo code cÅ© cá»§a báº¡n)
            mask_slice = np.rot90(mask_slice, k=1)
            mask_slice = np.flipud(mask_slice)

            mask_png = (mask_slice * 255).astype(np.uint8)

            save_name = f"slice_{i}.png"
            save_path = os.path.join(output_dir, save_name)

            cv2.imwrite(save_path, mask_png)

        print(f"   --> ÄÃ£ xong! LÆ°u táº¡i: {output_dir}")

    except Exception as e:
        print(f"   Lá»–I khi xá»­ lÃ½ file {nrrd_path}: {str(e)}")

# --- CHÆ¯Æ NG TRÃŒNH CHÃNH: QUÃ‰T Tá»° Äá»˜NG ---

if os.path.exists(SEGMENTATION_FOLDER):
    # Láº¥y danh sÃ¡ch táº¥t cáº£ cÃ¡c file trong thÆ° má»¥c
    all_files = os.listdir(SEGMENTATION_FOLDER)

    # Sáº¯p xáº¿p tÃªn file Ä‘á»ƒ cháº¡y theo thá»© tá»± (tÃ¹y chá»n)
    all_files.sort()

    print(f"TÃ¬m tháº¥y {len(all_files)} file trong thÆ° má»¥c. Báº¯t Ä‘áº§u xá»­ lÃ½...\n")

    for filename in all_files:
        # Chá»‰ xá»­ lÃ½ nhá»¯ng file cÃ³ Ä‘uÃ´i lÃ  .nrrd
        if filename.endswith('.nrrd'):

            # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘áº¿n file nrrd
            full_nrrd_path = os.path.join(SEGMENTATION_FOLDER, filename)

            # Táº¡o tÃªn thÆ° má»¥c Ä‘áº§u ra dá»±a trÃªn tÃªn file (bá» Ä‘uÃ´i .nrrd)
            # VÃ­ dá»¥: Brain.nrrd -> masks_Brain
            organ_name = filename.replace('.nrrd', '')
            output_dir = os.path.join(OUTPUT_ROOT, f"masks_{organ_name}")

            print(f"[*] Äang xá»­ lÃ½: {filename}")
            convert_nrrd_to_mask(full_nrrd_path, output_dir)
            print("-" * 50)

else:
    print(f"KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c nguá»“n: {SEGMENTATION_FOLDER}")

"""Chuáº©n hÃ³a Data

"""

import os
import glob
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

DATA_FOLDER_PATH = '/content/drive/MyDrive/Tep_moi'

IMG_SIZE = (256, 256)

class CT_PNG_Dataset(Dataset):
    def __init__(self, root_dir, is_train=True):
        """
        Args:
            root_dir (string): ÄÆ°á»ng dáº«n tá»›i folder Gá»C (vÃ­ dá»¥: Tep_moi)
            is_train (bool): True Ä‘á»ƒ báº­t cháº¿ Ä‘á»™ tÄƒng cÆ°á»ng dá»¯ liá»‡u (xoay/láº­t)
        """
        self.root_dir = root_dir

        path_lower = os.path.join(root_dir, "**", "*.png")
        path_upper = os.path.join(root_dir, "**", "*.PNG")

        self.image_paths = glob.glob(path_lower, recursive=True) + \
                           glob.glob(path_upper, recursive=True)

        if len(self.image_paths) == 0:
            print(f"Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y áº£nh nÃ o trong {root_dir} vÃ  cÃ¡c thÆ° má»¥c con!")
        else:
            print(f"ÄÃ£ tÃ¬m tháº¥y tá»•ng cá»™ng: {len(self.image_paths)} áº£nh.")
            print(f"   -> VÃ­ dá»¥ áº£nh Ä‘áº§u tiÃªn: {self.image_paths[0]}")

        self.is_train = is_train
        self.transform_aug = transforms.Compose([
            transforms.ToPILImage(),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ToTensor()
        ])

        self.transform_base = transforms.Compose([
            transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

        if image is None:
          return torch.zeros(1, 256, 256)
        image = cv2.resize(image, (256, 256))
        if self.is_train:
            image_tensor = self.transform_aug(image)
        else:
            image_tensor = self.transform_base(image)

        return image_tensor

try:
    train_dataset = CT_PNG_Dataset(root_dir=DATA_FOLDER_PATH, is_train=True)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

    data_iter = iter(train_loader)
    images = next(data_iter)

    print(f"Sá»‘ lÆ°á»£ng áº£nh tÃ¬m tháº¥y: {len(train_dataset)}")
    print(f"Shape cá»§a Batch Ä‘áº§u vÃ o mÃ´ hÃ¬nh: {images.shape}")
    print(f"GiÃ¡ trá»‹ pixel max: {images.max()}, min: {images.min()}")

    plt.figure(figsize=(10, 5))
    for i in range(4):
        if i >= len(images): break
        ax = plt.subplot(1, 4, i + 1)
        img_show = images[i].squeeze().numpy()
        plt.imshow(img_show, cmap='gray')
        plt.title(f"áº¢nh {i+1}")
        plt.axis('off')
    plt.show()

except Exception as e:
    print("CÃ³ lá»—i xáº£y ra (kháº£ nÄƒng do Ä‘Æ°á»ng dáº«n folder chÆ°a Ä‘Ãºng hoáº·c folder rá»—ng):")
    print(e)

import os
import cv2
import torch
import torchvision.transforms as transforms
from PIL import Image

# --- 1. Cáº¤U HÃŒNH ---
# Folder chá»©a CT gá»‘c
INPUT_CT_ROOT = '/content/drive/MyDrive/Tep_moi/oncologist/0522c0014'

OUTPUT_MRI_ROOT = '/content/drive/MyDrive/MRI_save'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. LOAD MÃ” HÃŒNH ÄÃƒ TRAIN ---

generator = UNetGenerator().to(DEVICE)
generator.load_state_dict(torch.load("/content/drive/MyDrive/generator_ct2mri_final.pth", map_location=DEVICE))
generator.eval()
print("ÄÃ£ load mÃ´ hÃ¬nh thÃ nh cÃ´ng!")

def process_single_image(img_path, save_path):
    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    if image is None: return
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((256, 256)),
        transforms.ToTensor()
    ])
    img_tensor = transform(image).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        fake_mri = generator(img_tensor)
    fake_mri = fake_mri.cpu().squeeze().numpy()
    fake_mri = (fake_mri * 255).astype('uint8')
    cv2.imwrite(save_path, fake_mri)

# --- 3. CHáº Y VÃ’NG Láº¶P QUÃ‰T TOÃ€N Bá»˜ FOLDER ---
print(f"--- Báº®T Äáº¦U CHUYá»‚N Äá»”I Dá»® LIá»†U ---")
print(f"Tá»«: {INPUT_CT_ROOT}")
print(f"Äáº¿n: {OUTPUT_MRI_ROOT}")
count = 0

for root, dirs, files in os.walk(INPUT_CT_ROOT):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            full_input_path = os.path.join(root, file)
            relative_path = os.path.relpath(full_input_path, INPUT_CT_ROOT)
            full_output_path = os.path.join(OUTPUT_MRI_ROOT, relative_path)
            os.makedirs(os.path.dirname(full_output_path), exist_ok=True)
            process_single_image(full_input_path, full_output_path)

            count += 1
            if count % 100 == 0:
                print(f"ÄÃ£ xá»­ lÃ½: {count} áº£nh...")

print(f"HOÃ€N Táº¤T! ÄÃ£ táº¡o ra {count} áº£nh MRI giáº£ láº­p.")
print(f"Kiá»ƒm tra táº¡i folder: {OUTPUT_MRI_ROOT}")

"""XÃ¢y dá»±ng kiáº¿n trÃºc GAN"""

import os
import glob
import numpy as np
import cv2
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.autograd import Variable

class MedicalImageDataset(Dataset):
    def __init__(self, root_dir, is_train=True):
        self.root_dir = root_dir
        self.is_train = is_train

        extensions = ["*.png", "*.PNG", "*.jpg", "*.jpeg", "*.JPG"]

        self.image_paths = []
        for ext in extensions:

            path_pattern = os.path.join(root_dir, "**", ext)
            files = glob.glob(path_pattern, recursive=True)
            self.image_paths.extend(files)

        if len(self.image_paths) == 0:
            print(f"Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y áº£nh nÃ o trong: {root_dir}")
        else:
            print(f"ÄÃ£ tÃ¬m tháº¥y {len(self.image_paths)} áº£nh táº¡i: {root_dir}")

        self.transform_aug = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((256, 256)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor()
        ])

        self.transform_base = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((256, 256)),
            transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

        if image is None:
            return torch.zeros(1, 256, 256)

        if self.is_train:
            return self.transform_aug(image)
        else:
            return self.transform_base(image)

class UNetGenerator(nn.Module):
    def __init__(self):
        super(UNetGenerator, self).__init__()

        def down_block(in_feat, out_feat, normalize=True):
            layers = [nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False)]
            if normalize: layers.append(nn.BatchNorm2d(out_feat))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return nn.Sequential(*layers)

        def up_block(in_feat, out_feat, dropout=0.0):
            layers = [
                nn.ConvTranspose2d(in_feat, out_feat, 4, 2, 1, bias=False),
                nn.BatchNorm2d(out_feat),
                nn.ReLU(inplace=True)
            ]
            if dropout: layers.append(nn.Dropout(dropout))
            return nn.Sequential(*layers)

        self.down1 = down_block(1, 64, normalize=False)
        self.down2 = down_block(64, 128)
        self.down3 = down_block(128, 256)
        self.down4 = down_block(256, 512)
        self.down5 = down_block(512, 512)
        self.down6 = down_block(512, 512)

        self.up1 = up_block(512, 512, dropout=0.5)
        self.up2 = up_block(1024, 512, dropout=0.5)
        self.up3 = up_block(1024, 256)
        self.up4 = up_block(512, 128)
        self.up5 = up_block(256, 64)

        self.final = nn.Sequential(
            nn.ConvTranspose2d(128, 1, 4, 2, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        d6 = self.down6(d5)

        u1 = self.up1(d6)
        u2 = self.up2(torch.cat([u1, d5], 1))
        u3 = self.up3(torch.cat([u2, d4], 1))
        u4 = self.up4(torch.cat([u3, d3], 1))
        u5 = self.up5(torch.cat([u4, d2], 1))

        return self.final(torch.cat([u5, d1], 1))

# --- 3. MÃ” HÃŒNH DISCRIMINATOR (PATCHGAN) ---
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, normalize=True):
            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]
            if normalize: layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *discriminator_block(1, 64, normalize=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1, bias=False)
        )

    def forward(self, img):
        return self.model(img)

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# --- 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---
DIR_CT = '/content/drive/MyDrive/Tep_moi/oncologist/0522c0014'
DIR_MRI = '/content/drive/MyDrive/MRI_save/oncologist/0522c0014'
DIR_MASK = '/content/drive/MyDrive/Tep_moiss/oncologist/0522c0014/masks_Brainstem'

# --- 2. HÃ€M TÃNH CHá»ˆ Sá» (Chá»‰ Accuracy) ---
def calculate_accuracy(pred, target, threshold=0.5):

    pred_bin = (pred > threshold).float()
    pred_flat = pred_bin.view(-1)
    target_flat = target.view(-1)

    tp = (pred_flat * target_flat).sum()
    fp = (pred_flat * (1 - target_flat)).sum()
    fn = ((1 - pred_flat) * target_flat).sum()
    tn = ((1 - pred_flat) * (1 - target_flat)).sum()

    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)

    return accuracy.item()

# --- 3. DATASET ---
class FusionDataset(Dataset):
    def __init__(self, ct_dir, mri_dir, mask_dir):
        self.ct_dir = ct_dir
        self.mri_dir = mri_dir
        self.mask_dir = mask_dir

        if os.path.exists(mask_dir):
            self.filenames = [f for f in os.listdir(mask_dir) if f.endswith('.png')]
            print(f" TÃ¬m tháº¥y {len(self.filenames)} máº«u dá»¯ liá»‡u.")
        else:
            self.filenames = []
            print(f" Lá»—i: KhÃ´ng tÃ¬m tháº¥y folder mask táº¡i {mask_dir}")

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        fname = self.filenames[idx]

        ct_path = os.path.join(self.ct_dir, fname)
        mri_path = os.path.join(self.mri_dir, fname)
        mask_path = os.path.join(self.mask_dir, fname)

        ct = cv2.imread(ct_path, cv2.IMREAD_GRAYSCALE)
        mri = cv2.imread(mri_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if ct is None: ct = np.zeros((256, 256), dtype=np.uint8)
        if mri is None: mri = np.zeros((256, 256), dtype=np.uint8)

        ct = cv2.resize(ct, (256, 256))
        mri = cv2.resize(mri, (256, 256))
        mask = cv2.resize(mask, (256, 256))

        mask = (mask > 127).astype(np.float32)

        to_tensor = transforms.ToTensor()
        return to_tensor(ct), to_tensor(mri), torch.from_numpy(mask).unsqueeze(0)

# --- 4. MODEL ---
class LateFusionModel(nn.Module):
    def __init__(self):
        super(LateFusionModel, self).__init__()
        # Encoder CT
        self.enc_ct = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2)
        )
        # Encoder MRI
        self.enc_mri = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2)
        )
        # Fusion
        self.fusion_conv = nn.Conv2d(64, 64, 3, padding=1)
        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 1, 1),
            nn.Sigmoid()
        )

    def forward(self, ct, mri):
        f1 = self.enc_ct(ct)
        f2 = self.enc_mri(mri)
        f_cat = torch.cat([f1, f2], dim=1)
        f_fused = self.fusion_conv(f_cat)
        return self.decoder(f_fused)

# --- 5. TRAINING LOOP ---
dataset = FusionDataset(DIR_CT, DIR_MRI, DIR_MASK)

if len(dataset) > 0:
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
    model = LateFusionModel().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.BCELoss()

    print("\n Báº®T Äáº¦U TRAINING...")
    NUM_EPOCHS = 100

    for epoch in range(NUM_EPOCHS):
        model.train()
        epoch_loss = 0
        epoch_acc = 0

        for ct, mri, mask in dataloader:
            ct, mri, mask = ct.to(DEVICE), mri.to(DEVICE), mask.to(DEVICE)

            optimizer.zero_grad()
            prediction = model(ct, mri)
            loss = criterion(prediction, mask)
            loss.backward()
            optimizer.step()

            # Chá»‰ tÃ­nh Accuracy
            acc = calculate_accuracy(prediction, mask)

            epoch_loss += loss.item()
            epoch_acc += acc

        # In káº¿t quáº£
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] | "
              f"Loss: {epoch_loss/len(dataloader):.4f} | "
              f"Acc: {epoch_acc/len(dataloader):.4f}")

    # LÆ°u model
    torch.save(model.state_dict(), "late_fusion_segmentation.pth")
    print("\n ÄÃ£ lÆ°u mÃ´ hÃ¬nh: late_fusion_segmentation.pth")
else:
    print(" Lá»—i: KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u!")

#Baseline Model (Chá»‰ dÃ¹ng CT)
import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

DIR_CT = '/content/drive/MyDrive/Tep_moi/oncologist/0522c0014'
DIR_MASK = '/content/drive/MyDrive/Tep_moiss/oncologist/0522c0014/masks_Brainstem'

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class DiceLoss(nn.Module):
    def __init__(self, smooth=1):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
    def forward(self, inputs, targets):
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        intersection = (inputs * targets).sum()
        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)
        return 1 - dice

model = BaselineUNet().to(DEVICE)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

criterion = DiceLoss()

print("\nBáº®T Äáº¦U TRAINING BASELINE (DICE LOSS)...")

for epoch in range(20):
    model.train()
    epoch_loss = 0
    for ct, mask in train_loader:
        ct, mask = ct.to(DEVICE), mask.to(DEVICE)

        optimizer.zero_grad()
        pred = model(ct)

        loss = criterion(pred, mask)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    print(f"Epoch {epoch+1}/20 | Loss: {epoch_loss/len(train_loader):.4f}")

print("\nÄANG CHáº Y TEST Láº I...")
model.eval()
dice_scores = []

with torch.no_grad():
    for ct, mask in test_loader:
        ct, mask = ct.to(DEVICE), mask.to(DEVICE)
        pred = model(ct)

        if mask.max() > 0:
            pred_bin = (pred > 0.5).float()
            intersection = (pred_bin * mask).sum()
            dice = (2. * intersection) / (pred_bin.sum() + mask.sum() + 1e-8)
            dice_scores.append(dice.item())

if len(dice_scores) > 0:
    avg_dice = sum(dice_scores) / len(dice_scores)
    print("="*40)
    print(f"Káº¾T QUáº¢ BASELINE Má»šI (DICE LOSS):")
    print(f"Average Dice Score: {avg_dice:.4f}")
    print("="*40)

# --- DATASET ÄÃƒ Lá»ŒC (Chá»‰ láº¥y áº£nh cÃ³ cÆ¡ quan) ---
class SingleChannelDataset(Dataset):
    def __init__(self, ct_dir, mask_dir):
        self.ct_dir = ct_dir
        self.mask_dir = mask_dir
        self.filenames = []

        # BÆ°á»›c lá»c dá»¯ liá»‡u (HÆ¡i lÃ¢u má»™t chÃºt lÃºc khá»Ÿi táº¡o)
        print("â³ Äang lá»c dá»¯ liá»‡u (Chá»‰ láº¥y lÃ¡t cáº¯t CÃ“ cÆ¡ quan)...")
        if os.path.exists(mask_dir):
            all_files = [f for f in os.listdir(mask_dir) if f.endswith('.png')]

            for f in all_files:
                mask_path = os.path.join(mask_dir, f)
                # Äá»c nhanh mask Ä‘á»ƒ kiá»ƒm tra
                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
                if mask is not None and np.max(mask) > 0: # Chá»‰ láº¥y náº¿u cÃ³ vá»‡t tráº¯ng
                    self.filenames.append(f)

        print(f"ÄÃ£ lá»c xong! CÃ²n láº¡i {len(self.filenames)} máº«u cháº¥t lÆ°á»£ng (trÃªn tá»•ng sá»‘ {len(all_files) if os.path.exists(mask_dir) else 0}).")

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        fname = self.filenames[idx]
        ct_path = os.path.join(self.ct_dir, fname)
        mask_path = os.path.join(self.mask_dir, fname)

        ct = cv2.imread(ct_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if ct is None: ct = np.zeros((256, 256), dtype=np.uint8)

        ct = cv2.resize(ct, (256, 256))
        mask = cv2.resize(mask, (256, 256))

        mask = (mask > 127).astype(np.float32)

        to_tensor = transforms.ToTensor()
        return to_tensor(ct), torch.from_numpy(mask).unsqueeze(0)

class BaselineUNet(nn.Module):
    def __init__(self):
        super(BaselineUNet, self).__init__()

        self.enc1 = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.Conv2d(32, 32, 3, padding=1), nn.ReLU())
        self.pool1 = nn.MaxPool2d(2)

        self.enc2 = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())
        self.pool2 = nn.MaxPool2d(2)

        self.bottleneck = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.Conv2d(128, 128, 3, padding=1), nn.ReLU())

        self.up2 = nn.Upsample(scale_factor=2)
        self.dec2 = nn.Sequential(nn.Conv2d(128 + 64, 64, 3, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, padding=1), nn.ReLU())

        self.up1 = nn.Upsample(scale_factor=2)
        self.dec1 = nn.Sequential(nn.Conv2d(64 + 32, 32, 3, padding=1), nn.ReLU(), nn.Conv2d(32, 32, 3, padding=1), nn.ReLU())

        self.final = nn.Sequential(nn.Conv2d(32, 1, 1), nn.Sigmoid())

    def forward(self, x):

        e1 = self.enc1(x)
        p1 = self.pool1(e1)

        e2 = self.enc2(p1)
        p2 = self.pool2(e2)

        b = self.bottleneck(p2)

        d2 = self.up2(b)
        d2 = torch.cat((d2, e2), dim=1)
        d2 = self.dec2(d2)

        d1 = self.up1(d2)
        d1 = torch.cat((d1, e1), dim=1)
        d1 = self.dec1(d1)

        return self.final(d1)

# --- 4. HÃ€M TÃNH METRICS ---
def calculate_metrics(pred, target, threshold=0.5):
    pred_bin = (pred > threshold).float()
    intersection = (pred_bin * target).sum()
    dice = (2. * intersection) / (pred_bin.sum() + target.sum() + 1e-8)
    return dice.item()

# --- 5. CHáº Y THá»°C NGHIá»†M ---
full_dataset = SingleChannelDataset(DIR_CT, DIR_MASK)

if len(full_dataset) > 0:
    # 1. Chia táº­p Train/Test (80% train, 20% test)
    train_size = int(0.8 * len(full_dataset))
    test_size = len(full_dataset) - train_size
    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch 1 Ä‘á»ƒ test tá»«ng áº£nh

    print(f"Tá»•ng dá»¯ liá»‡u: {len(full_dataset)}")
    print(f"Train set: {len(train_dataset)} | Test set: {len(test_dataset)}")

    # 2. Huáº¥n luyá»‡n
    model = BaselineUNet().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.BCELoss()

    print("\nBáº®T Äáº¦U TRAINING BASELINE (CHá»ˆ CT)...")
    for epoch in range(20): # Train nhanh 20 epoch Ä‘á»ƒ so sÃ¡nh
        model.train()
        epoch_loss = 0
        for ct, mask in train_loader:
            ct, mask = ct.to(DEVICE), mask.to(DEVICE)
            optimizer.zero_grad()
            pred = model(ct)
            loss = criterion(pred, mask)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        print(f"Epoch {epoch+1}/20 | Loss: {epoch_loss/len(train_loader):.4f}")

    # 3. ÄÃ¡nh giÃ¡ trÃªn táº­p Test (QUAN TRá»ŒNG)
    print("\nğŸ“Š ÄANG CHáº Y TRÃŠN Táº¬P TEST...")
    model.eval()
    dice_scores = []

    with torch.no_grad():
        for ct, mask in test_loader:
            ct, mask = ct.to(DEVICE), mask.to(DEVICE)
            pred = model(ct)

            # Chá»‰ tÃ­nh Dice náº¿u mask cÃ³ cÆ¡ quan (Ä‘á»ƒ trÃ¡nh nhiá»…u do áº£nh Ä‘en)
            if mask.max() > 0:
                dice = calculate_metrics(pred, mask)
                dice_scores.append(dice)

    if len(dice_scores) > 0:
        avg_dice = sum(dice_scores) / len(dice_scores)
        print("="*40)
        print(f"Káº¾T QUáº¢ BASELINE (CT ONLY):")
        print(f"Average Dice Score: {avg_dice:.4f}")
        print("="*40)
    else:
        print("Táº­p test toÃ n áº£nh Ä‘en, khÃ´ng tÃ­nh Ä‘Æ°á»£c Dice chÃ­nh xÃ¡c.")

else:
    print("KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u!")

# --- 5. HIá»‚N THá»Š Káº¾T QUáº¢ (CHá»ŒN Lá»ŒC) ---
model.eval()
found_mask = False
print("Äang tÃ¬m lÃ¡t cáº¯t cÃ³ cÆ¡ quan Ä‘á»ƒ hiá»ƒn thá»‹...")

with torch.no_grad():
    for ct, mri, mask in dataloader:
        if mask.max() > 0:

            ct, mri = ct.to(DEVICE), mri.to(DEVICE)
            pred = model(ct, mri)

            for i in range(ct.size(0)):
                if mask[i].max() > 0:
                    img_ct = ct[i].cpu().squeeze()
                    img_mri = mri[i].cpu().squeeze()
                    img_mask = mask[i].cpu().squeeze()
                    img_pred = pred[i].cpu().squeeze()

                    plt.figure(figsize=(16, 5))

                    plt.subplot(1, 4, 1)
                    plt.imshow(img_ct, cmap='gray')
                    plt.title("Input CT (KÃªnh 1)")
                    plt.axis('off')

                    plt.subplot(1, 4, 2)
                    plt.imshow(img_mri, cmap='gray')
                    plt.title("Synthetic MRI (KÃªnh 2)")
                    plt.axis('off')

                    plt.subplot(1, 4, 3)
                    plt.imshow(img_mask, cmap='gray')
                    plt.title("True Mask (ÄÃ¡p Ã¡n)")
                    plt.axis('off')

                    plt.subplot(1, 4, 4)
                    plt.imshow(img_pred > 0.5, cmap='gray')
                    plt.title("AI Predicted (Káº¿t quáº£)")
                    plt.axis('off')

                    plt.show()

                    found_mask = True
                    break

        if found_mask:
            break

if not found_mask:
    print("KhÃ´ng tÃ¬m tháº¥y áº£nh nÃ o cÃ³ Mask trong lÆ°á»£t random nÃ y. HÃ£y cháº¡y láº¡i cell nÃ y láº§n ná»¯a!")

"""code má»›i"""

import os
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import glob
import random
from PIL import Image

# --- Cáº¤U HÃŒNH ---
IMG_SIZE = 256
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = '/content/drive/MyDrive/unet_best.pth'

# ğŸ‘‡ DÃN ÄÆ¯á»œNG DáºªN THÆ¯ Má»¤C áº¢NH Cá»¦A Báº N VÃ€O GIá»®A 2 Dáº¤U NHÃY DÆ¯á»šI ÄÃ‚Y ğŸ‘‡
TEST_DIR = '/content/drive/MyDrive/anh_ct/oncologist/0522c0014'
# (VÃ­ dá»¥: /content/anh_ct/oncologist/0522c0014 - Báº¡n nhá»› thay cho Ä‘Ãºng nhÃ©)

# --- KIáº¾N TRÃšC MODEL (GIá»® NGUYÃŠN Báº¢N PHá»¤C CHáº¾ ÄÃƒ CHáº Y ÄÆ¯á»¢C) ---
class ConvBlock(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c),
            nn.ReLU(inplace=True)
        )
    def forward(self, x):
        return self.conv(x)

class RestoredUNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.d1 = ConvBlock(1, 64)
        self.d2 = ConvBlock(64, 128)
        self.d3 = ConvBlock(128, 256)
        self.bot = ConvBlock(256, 512)
        self.pool = nn.MaxPool2d(2, 2)

        self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.u1 = ConvBlock(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.u2 = ConvBlock(256, 128)

        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.u3 = ConvBlock(128, 64)

        self.final = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x):
        d1 = self.d1(x)
        p1 = self.pool(d1)
        d2 = self.d2(p1)
        p2 = self.pool(d2)
        d3 = self.d3(p2)
        p3 = self.pool(d3)
        bot = self.bot(p3)

        x = self.up1(bot)
        if x.shape != d3.shape: x = F.interpolate(x, size=d3.shape[2:])
        x = torch.cat([d3, x], dim=1)
        x = self.u1(x)

        x = self.up2(x)
        if x.shape != d2.shape: x = F.interpolate(x, size=d2.shape[2:])
        x = torch.cat([d2, x], dim=1)
        x = self.u2(x)

        x = self.up3(x)
        if x.shape != d1.shape: x = F.interpolate(x, size=d1.shape[2:])
        x = torch.cat([d1, x], dim=1)
        x = self.u3(x)

        return self.final(x)

# --- CHáº Y TEST Tá»° Äá»˜NG Tá»ª THÆ¯ Má»¤C ---
def run_test_folder():
    # 1. Load Model
    print(f"ğŸ”„ Äang náº¡p model cÅ© tá»« {MODEL_PATH}...")
    try:
        model = RestoredUNet().to(DEVICE)
        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
        model.eval()
        print("âœ… Náº¡p model thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ Lá»—i model: {str(e)}")
        return

    # 2. QuÃ©t áº£nh trong thÆ° má»¥c
    print(f"ğŸ“‚ Äang tÃ¬m áº£nh trong: {TEST_DIR}")
    files = glob.glob(os.path.join(TEST_DIR, '*.png')) + glob.glob(os.path.join(TEST_DIR, '*.jpg'))

    if len(files) == 0:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file áº£nh nÃ o (.png, .jpg) trong thÆ° má»¥c nÃ y!")
        print("ğŸ‘‰ Báº¡n hÃ£y kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n xem Ä‘Ãºng chÆ°a nhÃ©.")
        return

    # 3. Chá»n ngáº«u nhiÃªn 1 áº£nh Ä‘á»ƒ test
    test_file = random.choice(files)
    print(f"âœ¨ ÄÃ£ chá»n ngáº«u nhiÃªn áº£nh: {os.path.basename(test_file)}")

    # 4. Xá»­ lÃ½ vÃ  Dá»± Ä‘oÃ¡n
    img_origin = cv2.imread(test_file, 0)
    if img_origin is None:
        print("âŒ Lá»—i Ä‘á»c file áº£nh (file cÃ³ thá»ƒ bá»‹ há»ng).")
        return

    img_resized = cv2.resize(img_origin, (IMG_SIZE, IMG_SIZE))
    img_tensor = torch.tensor(img_resized/255.0, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        pred = model(img_tensor)
        pred = torch.sigmoid(pred)
        pred_mask = (pred > 0.5).float().cpu().numpy()[0, 0]

    # 5. Váº½ káº¿t quáº£
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 3, 1)
    plt.title("áº¢nh CT Gá»‘c")
    plt.imshow(img_resized, cmap='gray')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.title("AI Dá»± Ä‘oÃ¡n (Mask)")
    plt.imshow(pred_mask, cmap='gray')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.title("Káº¿t quáº£ chá»“ng lá»›p")
    plt.imshow(img_resized, cmap='gray')
    plt.imshow(pred_mask, cmap='jet', alpha=0.5)
    plt.axis('off')

    plt.show()

if __name__ == "__main__":
    run_test_folder()

import matplotlib.pyplot as plt
import pandas as pd

# Äá»c file log cá»§a báº¡n
# Äáº£m báº£o file log_train1.txt náº±m cÃ¹ng thÆ° má»¥c vá»›i file code nÃ y
df = pd.read_csv('/content/drive/MyDrive/log_train1.txt')

# Cáº¥u hÃ¬nh biá»ƒu Ä‘á»“
plt.figure(figsize=(12, 5))

# --- BIá»‚U Äá»’ 1: LOSS ---
plt.subplot(1, 2, 1)
plt.plot(df['Epoch'], df['Train_Loss'], label='Train Loss', color='blue', linewidth=2)
plt.plot(df['Epoch'], df['Val_Loss'], label='Val Loss', color='red', linestyle='--', linewidth=2)
plt.title('QuÃ¡ trÃ¬nh giáº£m hÃ m máº¥t mÃ¡t (Loss)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# --- BIá»‚U Äá»’ 2: DICE & ACCURACY ---
plt.subplot(1, 2, 2)
plt.plot(df['Epoch'], df['Val_Dice'], label='Dice Score', color='green', linewidth=2)
# Accuracy cao quÃ¡ nÃªn ta váº½ trá»¥c phá»¥ hoáº·c chá»‰ váº½ Dice Ä‘á»ƒ nhÃ¬n cho rÃµ dao Ä‘á»™ng
plt.plot(df['Epoch'], df['Val_Acc'], label='Accuracy', color='orange', linestyle='--', linewidth=2)
plt.title('Hiá»‡u nÄƒng mÃ´ hÃ¬nh (Dice & Accuracy)')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.legend()
plt.grid(True, alpha=0.3)

# LÆ°u áº£nh
plt.tight_layout()
plt.savefig('ket_qua_train.png', dpi=300)
print("ÄÃ£ váº½ xong biá»ƒu Ä‘á»“! Má»Ÿ file 'ket_qua_train.png' Ä‘á»ƒ xem.")
plt.show()

import os
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import glob
import re
import random

# --- Cáº¤U HÃŒNH ---
IMG_SIZE = 256
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

MODEL_PATH = '/content/drive/MyDrive/unet_best_dice.pth'
CT_DIR = '/content/drive/MyDrive/anh_ct'
MRI_DIR = '/content/drive/MyDrive/MRI_save'
MASK_DIR = '/content/drive/MyDrive/anh_mask'

# --- 1. MODEL DEFINITION ---
class TwoChannelUNet(nn.Module):
    def __init__(self):
        super().__init__()
        def CBR(in_c, out_c):
            return nn.Sequential(
                nn.Conv2d(in_c, out_c, 3, padding=1),
                nn.BatchNorm2d(out_c),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_c, out_c, 3, padding=1),
                nn.BatchNorm2d(out_c),
                nn.ReLU(inplace=True)
            )
        self.enc1 = CBR(2, 64)
        self.pool1 = nn.MaxPool2d(2)
        self.enc2 = CBR(64, 128)
        self.pool2 = nn.MaxPool2d(2)
        self.enc3 = CBR(128, 256)
        self.pool3 = nn.MaxPool2d(2)
        self.bottle = CBR(256, 512)
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = CBR(512, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = CBR(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = CBR(128, 64)
        self.final = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        e3 = self.enc3(self.pool2(e2))
        b = self.bottle(self.pool3(e3))
        d3 = self.dec3(torch.cat((self.up3(b), e3), dim=1))
        d2 = self.dec2(torch.cat((self.up2(d3), e2), dim=1))
        d1 = self.dec1(torch.cat((self.up1(d2), e1), dim=1))
        return self.final(d1)

# --- 2. HÃ€M QUÃ‰T FILE THÃ”NG MINH (SMART SCAN) ---
def get_random_pair():
    print("ğŸ”„ Äang quÃ©t dá»¯ liá»‡u (Smart Scan)...")

    def extract_id(filename):
        numbers = re.findall(r'\d+', filename)
        return int(numbers[-1]) if numbers else None

    # HÃ m tÃ¬m file Ä‘á»‡ quy (quÃ©t sÃ¢u vÃ o thÆ° má»¥c con)
    def index_files(root_dir):
        file_map = {}
        # TÃ¬m táº¥t cáº£ cÃ¡c Ä‘uÃ´i áº£nh phá»• biáº¿n
        extensions = ['*.png', '*.jpg', '*.jpeg', '*.tif', '*.bmp']
        files = []
        for ext in extensions:
            # recursive=True giÃºp tÃ¬m cáº£ trong thÆ° má»¥c con
            files.extend(glob.glob(os.path.join(root_dir, '**', ext), recursive=True))

        for f in files:
            fid = extract_id(os.path.basename(f))
            if fid is not None:
                file_map[fid] = f
        return file_map, len(files)

    # QuÃ©t tá»«ng thÆ° má»¥c
    ct_map, n_ct = index_files(CT_DIR)
    mri_map, n_mri = index_files(MRI_DIR)
    mask_map, n_mask = index_files(MASK_DIR)

    print(f"   -> TÃ¬m tháº¥y: {n_ct} CT, {n_mri} MRI, {n_mask} Mask")

    # TÃ¬m ID chung
    common_ids = list(set(ct_map.keys()) & set(mri_map.keys()) & set(mask_map.keys()))

    if not common_ids:
        print("âŒ VáºªN KHÃ”NG TÃŒM THáº¤Y! HÃ£y kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n thÆ° má»¥c CT_DIR, MRI_DIR.")
        print(f"   ÄÆ°á»ng dáº«n hiá»‡n táº¡i Ä‘ang set lÃ : {CT_DIR}")
        return None, None, None

    # Bá»‘c ngáº«u nhiÃªn 1 ID
    random_id = random.choice(common_ids)
    print(f"âœ¨ ÄÃ£ chá»n ngáº«u nhiÃªn ID: {random_id}")
    return ct_map[random_id], mri_map[random_id], mask_map[random_id]

# --- 3. Dá»° ÄOÃN & Váº¼ áº¢NH ---
def predict_and_show():
    # 1. Láº¥y áº£nh
    ct_path, mri_path, mask_path = get_random_pair()
    if ct_path is None: return

    print(f"   - CT: {ct_path}")
    print(f"   - MRI: {mri_path}")

    # 2. Load Model
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file model: {MODEL_PATH}")
        print("   -> Báº¡n nhá»› sá»­a biáº¿n MODEL_PATH á»Ÿ Ä‘áº§u file cho Ä‘Ãºng tÃªn file model nhÃ©!")
        return

    model = TwoChannelUNet().to(DEVICE)
    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    except Exception as e:
        print(f"âŒ Lá»—i khi load model: {e}")
        return

    model.eval()

    # 3. Xá»­ lÃ½ áº£nh Ä‘áº§u vÃ o
    ct_img = cv2.imread(ct_path, 0)
    mri_img = cv2.imread(mri_path, 0)

    ct_resized = cv2.resize(ct_img, (IMG_SIZE, IMG_SIZE)) / 255.0
    mri_resized = cv2.resize(mri_img, (IMG_SIZE, IMG_SIZE)) / 255.0

    ct_tensor = torch.tensor(ct_resized, dtype=torch.float32).unsqueeze(0)
    mri_tensor = torch.tensor(mri_resized, dtype=torch.float32).unsqueeze(0)
    input_tensor = torch.cat([ct_tensor, mri_tensor], dim=0).unsqueeze(0).to(DEVICE)

    # 4. Dá»± Ä‘oÃ¡n
    with torch.no_grad():
        output = model(input_tensor)
        pred_mask = torch.sigmoid(output).cpu().numpy()[0, 0]
        pred_binary = (pred_mask > 0.5).astype(np.float32)

    # 5. Load Mask tháº­t
    gt_mask = cv2.imread(mask_path, 0)
    gt_mask = cv2.resize(gt_mask, (IMG_SIZE, IMG_SIZE)) / 255.0

    # 6. Váº½ vÃ  LÆ°u
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 4, 1); plt.imshow(ct_resized, cmap='gray'); plt.title("CT Input"); plt.axis('off')
    plt.subplot(1, 4, 2); plt.imshow(mri_resized, cmap='gray'); plt.title("MRI Input"); plt.axis('off')
    plt.subplot(1, 4, 3); plt.imshow(gt_mask, cmap='gray'); plt.title("Ground Truth"); plt.axis('off')
    plt.subplot(1, 4, 4); plt.imshow(pred_binary, cmap='gray'); plt.title("Prediction"); plt.axis('off')

    save_name = f"result_id_{os.path.basename(ct_path)}"
    plt.savefig(save_name)
    print(f"âœ… XONG! Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o file: {save_name}")

if __name__ == "__main__":
    predict_and_show()

"""áº¢nh MRI

"""

import os
import cv2
import numpy as np
from tqdm import tqdm  # ThÆ° viá»‡n táº¡o thanh tiáº¿n trÃ¬nh cho Ä‘áº¹p

# ==========================================
# 1. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# ==========================================
# ThÆ° má»¥c gá»‘c chá»©a áº£nh CT (Nguá»“n)
INPUT_ROOT = '/content/drive/MyDrive/anh_ct'

# ThÆ° má»¥c gá»‘c Ä‘á»ƒ lÆ°u áº£nh MRI giáº£ láº­p (ÄÃ­ch)
OUTPUT_ROOT = '/content/drive/MyDrive/MRI_save'

# CÃ¡c Ä‘uÃ´i file áº£nh cháº¥p nháº­n
VALID_EXTS = ('.png', '.jpg', '.jpeg', '.bmp')

# ==========================================
# 2. HÃ€M Xá»¬ LÃ CHÃNH
# ==========================================
def create_mri_dataset():
    count = 0
    print(f"ğŸš€ Báº¯t Ä‘áº§u quÃ©t vÃ  chuyá»ƒn Ä‘á»•i áº£nh tá»«: {INPUT_ROOT}")
    print(f"ğŸ“‚ áº¢nh káº¿t quáº£ sáº½ lÆ°u táº¡i: {OUTPUT_ROOT}\n")

    # Duyá»‡t qua táº¥t cáº£ cÃ¡c thÆ° má»¥c con
    for root, dirs, files in os.walk(INPUT_ROOT):
        for file in files:
            if file.lower().endswith(VALID_EXTS):
                # --- A. XÃC Äá»ŠNH ÄÆ¯á»œNG DáºªN ---
                # ÄÆ°á»ng dáº«n file gá»‘c
                src_path = os.path.join(root, file)

                # TÃ­nh toÃ¡n Ä‘Æ°á»ng dáº«n Ä‘Ã­ch (giá»¯ nguyÃªn cáº¥u trÃºc thÆ° má»¥c)
                # VÃ­ dá»¥: root = .../anh_ct/patient1 -> rel_path = patient1
                rel_path = os.path.relpath(root, INPUT_ROOT)

                # ÄÆ°á»ng dáº«n thÆ° má»¥c Ä‘Ã­ch tÆ°Æ¡ng á»©ng
                dest_dir = os.path.join(OUTPUT_ROOT, rel_path)

                # Náº¿u thÆ° má»¥c Ä‘Ã­ch chÆ°a cÃ³ thÃ¬ táº¡o má»›i
                if not os.path.exists(dest_dir):
                    os.makedirs(dest_dir)

                # ÄÆ°á»ng dáº«n file Ä‘Ã­ch
                dest_path = os.path.join(dest_dir, file)

                # --- B. Xá»¬ LÃ áº¢NH (THUáº¬T TOÃN) ---
                try:
                    # 1. Äá»c áº£nh CT (Grayscale)
                    ct_img = cv2.imread(src_path, 0)

                    if ct_img is None:
                        continue # Bá» qua náº¿u lá»—i Ä‘á»c file

                    # 2. Táº¡o MRI giáº£ láº­p (Invert + Blur)
                    # CÃ´ng thá»©c: MRI = 255 - CT
                    mri_fake = 255 - ct_img

                    # LÃ m má» nháº¹ (Gaussian Blur)
                    mri_fake = cv2.GaussianBlur(mri_fake, (3,3), 0)

                    # 3. LÆ°u áº£nh
                    cv2.imwrite(dest_path, mri_fake)
                    count += 1

                    # In ra tiáº¿n trÃ¬nh (má»—i 100 áº£nh bÃ¡o 1 láº§n cho Ä‘á»¡ rá»‘i)
                    if count % 100 == 0:
                        print(f"âœ… ÄÃ£ xá»­ lÃ½ {count} áº£nh... (Äang á»Ÿ: {rel_path})")

                except Exception as e:
                    print(f"âŒ Lá»—i file {file}: {e}")

    print("\n" + "="*40)
    print(f"ğŸ‰ HOÃ€N Táº¤T! Tá»”NG Cá»˜NG ÄÃƒ Táº O: {count} áº¢NH MRI GIáº¢ Láº¬P.")
    print(f"ğŸ‘‰ Báº¡n hÃ£y vÃ o folder '{OUTPUT_ROOT}' Ä‘á»ƒ kiá»ƒm tra nhÃ©.")
    print("="*40)

# ==========================================
# 3. CHáº Y CHÆ¯Æ NG TRÃŒNH
# ==========================================
if __name__ == "__main__":
    if os.path.exists(INPUT_ROOT):
        create_mri_dataset()
    else:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c nguá»“n: {INPUT_ROOT}")
        print("Báº¡n kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n Google Drive xem Ä‘Ãºng chÆ°a nhÃ©.")